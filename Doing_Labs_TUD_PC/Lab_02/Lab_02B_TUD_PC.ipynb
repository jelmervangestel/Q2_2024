{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEN122A Statistical Analysis of Choice Behaviour \n",
    "\n",
    "## `Session Lab 02B:`\n",
    "## `The Mixed Logit model`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**Q2 2024**<br>\n",
    "**Instructor:** Sander van Cranenburgh<br>\n",
    "**TA:**  Gabriel Nova <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Instructions`\n",
    "\n",
    "**Lab sessions aim to:**<br>\n",
    "* Illustrate how models and theory discussed in the classroom work out in practice.\n",
    "* Help you gather hands-on modelling and data analysis skills.\n",
    "\n",
    "\n",
    "**Lab sessions are:**<br>\n",
    "* Learning environments where you work with Python and get support from TA and fellow students.\n",
    "* Not graded and do not have to be submitted.\n",
    "* A good preparation for the graded partial exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Application: Estimating the Value of Travel Time`\n",
    "\n",
    "In this lab session, we will investigate the \"Value of Travel Time\" (VTT) distribution. The VTT of a traveller reflects the amount of money the traveller is **willing to pay** to reduce their travel time. The VTT is used to determine the benefits of new infrastructure projects. As travel time savings are the dominant and most salient benefits of new infrastructure, accurate inference of the distribution of the VTT is crucial for a rigorous underpinning of policy decisions. <br>\n",
    "\n",
    "During this lab, we will apply Mixed Logit choice models. We aim to uncover how tastes for travel time and travel cost are distributed in the population. Most of the analyses in this lab session are carried out in the so-called willingness-to-pay space. Willingness-to-pay space facilitates the inference of the VTT distribution.<br>\n",
    "\n",
    "For this study, we will use Stated Choice (SC) data (`Norway_VTT_2009.csv`) collected in 2009 to compute the Norwegian VTT. In this SC experiment, respondents faced nine choice tasks involving two alternatives and two attributes (travel cost and travel time). The data set consists of 5,832 participants, resulting in a total of 52,488 choice observations. The figure below shows one of the choice tasks (note that for the purposes of illustration we converted the currency unit (Kronor) into euros).\n",
    "\n",
    "![SC](data/sc_experiment.png)\n",
    "\n",
    "**`Learning objectives lab session 02B`**\n",
    "\n",
    "After completing the following exercises, you will be able to:\n",
    "* Estimate Mixed Logit models that account for panel data\n",
    "* Discuss the impact of the number of draws on the modelling outcomes\n",
    "\n",
    "\n",
    "**`This lab consists of 2 parts and has 2 exercises`**\n",
    "\n",
    "**Part 1**: The Panel Mixed Logit model\n",
    "\n",
    "- Exercise 1: \"Panel ML model with log-normally distributed VTT\"\n",
    "\n",
    "**Part 2**: Impact of the number of draws on modelling outcomes\n",
    "\n",
    "- Exercise 2: \"Impact of the number of draws\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Import packages`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will import all the libraries that we will use in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biogeme\n",
    "import os\n",
    "base_path = os.getcwd()\n",
    "source_path = base_path + '\\\\source'\n",
    "os.chdir(source_path)\n",
    "\n",
    "from biogeme import biogeme \n",
    "import biogeme.database as db\n",
    "import biogeme.biogeme as bio\n",
    "import biogeme.biogeme_logging as blog\n",
    "from biogeme import models\n",
    "from biogeme.expressions import Beta, Variable, bioDraws, log, MonteCarlo, exp, bioMultSum, exp\n",
    "\n",
    "\n",
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "from pathlib import Path\n",
    "from scipy.stats import norm, lognorm\n",
    "import seaborn\n",
    "\n",
    "# Pandas setting to show all columns when displaying a pandas dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "os.chdir(base_path)\n",
    "if os.getcwd() == base_path:\n",
    "    print(\"Every package loaded fine\")\n",
    "else:\n",
    "    print(\"Restart the kernel and try again. If not, call TA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We invoke a so-called `logger` which enables us to see the progress during estimation.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger, if it has not been initialized yet\n",
    "try:\n",
    "    logger\n",
    "except NameError:    \n",
    "    logger = blog.get_screen_logger(level=blog.INFO)\n",
    "    print('Logger has been initialised')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1. Load and explore the data set` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same data set as in lab session 2A. So,we load the data and process it similarly as in lab session 2A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data set\n",
    "data_path = Path('data/Norway_VTT_2009.csv')\n",
    "df = pd.read_table(data_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Keep only entries purpose == 5 (Long distance trips) & Mode == 1 (Car)\n",
    "df = df.loc[(df['Purpose'] == 5) & (df['Mode'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert the monetary unit to euros\n",
    "NOK2euro_exchange_rate = 9\n",
    "df[['CostL','CostR']] = df[['CostL','CostR']] .div(NOK2euro_exchange_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1. The Panel Mixed Logit model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we have worked on the assumption that each choice observation is uncorrelated with all other choice observations. However, this data set contains multiple choices per respondent. In the ML modelling framework, we can also account for correlation in unobserved utility **across observations** of the same individual if we specify it as a panel ML model. In the panel ML model, the likelihood of the sequence of choices *t* = 1..*T* of an individual *n* is given by:  \n",
    "\n",
    "$L_n(i_1,...,i_{T})(\\beta_n|\\sigma) = \\int_{\\beta_n}\\Pi_{t=1}^T     P_{n}(i_t|\\beta_n) f(\\beta_n|\\sigma)d\\beta_n$\n",
    "\n",
    "This likelihood does not have a closed-form expression. Therefore, as before, it needs to be approximated using simulation. Let's re-estimate the ML model assuming a normally distributed VTT distribution while accounting for panel structure. To do this, we first need to convert the data set into a so-called wide data format. In a wide format data set, each row contains all the choices belonging to an individual. Conveniently, Biogeme has a built-in function to do this (but, rather inconveniently, the names of the columns still need to be renamed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1.1. Preparing a wide Biogeme database for estimating panel ML model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we transform our data set into a wide format, and create a new Biogeme database object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Biogeme database object\n",
    "biodata = db.Database('Norway2009VTT', df)\n",
    "\n",
    "# Tell Biogeme which variable is the identifier of the individuals\n",
    "biodata.panel('RespID')\n",
    "\n",
    "# Calculate the number of observations per individual\n",
    "obs_per_ind = biodata.data['RespID'].value_counts().unique()[0]\n",
    "print(f'Number of observations per individual: {obs_per_ind}')\n",
    "\n",
    "# Use biogeme's \"generateFlatPanelDataFrame to create a wide database in which each row corresponds to one individual\n",
    "df_wide = biodata.generate_flat_panel_dataframe(identical_columns=None)\n",
    "\n",
    "# Rename the columns, such that they run from columnname_{0} to columnname_{n} \n",
    "renumbered_columns = {col: f'{col.split(\"_\")[1]}_{int(col.split(\"_\")[0])-1}' if len(col.split(\"_\")) == 2 else col for col in df_wide.columns}\n",
    "\n",
    "# Rename the columns using the dictionary\n",
    "df_wide.rename(columns=renumbered_columns, inplace=True)\n",
    "\n",
    "# Create Biogeme database object\n",
    "biodata_wide = db.Database('Norway2009VTT_wide', df_wide)\n",
    "\n",
    "# Show the first rows of the wide database\n",
    "print(f'The wide dataset has a shape of {biodata_wide.data.shape}')\n",
    "biodata_wide.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1.2. Panel ML model with normally distributed VTT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the model a name\n",
    "model_name = 'Panel ML WTP space with normally distributed vtt'\n",
    "\n",
    "# Parameters definition enabling the construction of random parameters\n",
    "vtt       = Beta('vtt',       0.4, None, None, 0)\n",
    "B_tc      = Beta('b_tc',     -0.4, None, None, 0)    \n",
    "sigma_vtt = Beta('sigma_vtt ',  2, None, None, 0)\n",
    "\n",
    "# Construction of random parameters   \n",
    "vtt_rnd = vtt + sigma_vtt * bioDraws('vtt_rnd', 'NORMAL_HALTON2')\n",
    "\n",
    "# Definition of the utility functions\n",
    "# Note that we use list comprehension to create a list of utility functions for all observations of an individual \n",
    "V_L = [B_tc * (Variable(f'CostL_{q}') + vtt_rnd * Variable(f'TimeL_{q}')) for q in range(obs_per_ind)]\n",
    "V_R = [B_tc * (Variable(f'CostR_{q}') + vtt_rnd * Variable(f'TimeR_{q}')) for q in range(obs_per_ind)]\n",
    "\n",
    "# Create a dictionary to list the utility functions with the numbering of alternatives\n",
    "# Note that we use list comprehension to create a list of dictionaries\n",
    "V = [{1: V_L[q], 2: V_R[q]} for q in range(obs_per_ind)]\n",
    "           \n",
    "# Create a dictionary to describe the availability conditions of each alternative\n",
    "av = {1:1, 2:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conditional probability of the chosen alternative is a logit\n",
    "condProb = [models.loglogit(V[q], av, Variable(f'Chosen_{q}')) for q in range(obs_per_ind)] \n",
    "\n",
    "# Take the product of the conditional probabilities\n",
    "condprobIndiv = exp(bioMultSum(condProb))   # exp to convert from logP to P again\n",
    "\n",
    "# The unconditional probability is obtained by simulation\n",
    "uncondProb = MonteCarlo(condprobIndiv)\n",
    "\n",
    "# The Log-likelihood is the log of the unconditional probability\n",
    "LL = log(uncondProb)\n",
    "\n",
    "# Create the Biogeme estimation object containing the data and the model\n",
    "num_draws = 100\n",
    "biogeme = bio.BIOGEME(biodata_wide , LL, number_of_draws=num_draws)\n",
    "\n",
    "# Compute the null loglikelihood for reporting\n",
    "# Note that we need to compute it manually, as biogeme does not do this for panel data\n",
    "biogeme.nullLogLike = len(biodata_wide.data)*np.log(1/2)*obs_per_ind\n",
    "\n",
    "# Set reporting levels\n",
    "biogeme.generate_pickle = False\n",
    "biogeme.generate_html = False\n",
    "biogeme.save_iterations = False\n",
    "biogeme.modelName = model_name                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the parameters and print the results\n",
    "results = biogeme.estimate()\n",
    "print(results.print_general_statistics())\n",
    "\n",
    "# Get the results in a pandas table\n",
    "beta_hat = results.get_estimated_parameters()\n",
    "print(beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the value of travel time\n",
    "VTT_WTP_ML_PANEL_normal = 60 * beta_hat.loc['vtt']['Value']\n",
    "print(f'Value of travel time Panel ML model in WTP space:  â‚¬{VTT_WTP_ML_PANEL_normal:.2f} per hour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Exercise 1: Panel ML with log-normally distributed VTT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **you** will estimate a ML model under the assumption that the VTT is log-normally distributed, while accounting for panel effects.<br>\n",
    "\n",
    "To do so, copy the code from the Panel ML model in WTP space with normally distributed VTT, and create the log-normally distributed random parameter (as you have done in exercise 1 of lab_session 2A).<br>  \n",
    "Estimate this model and interpret the results.<br>\n",
    "\n",
    "\n",
    "`A` Compare the log-likelihood of the ML models with the log-normally distributed VTTs, which do and do not account for the panel effect. Which model fits better?<br>\n",
    "\n",
    "`B` Compute the mean of the VTT for the Panel ML model with the log-normally distributed VTT and compare it with the non-panel model. Has it changed?<br>\n",
    "\n",
    "`C`  i. Print the recovered mean VTTs of the models we have estimated below each other.<br>\n",
    "* MNL model<br>\n",
    "* ML model with Normal distribution in utility space<br>\n",
    "* ML model with Normal distribution in wtp space<br>\n",
    "* ML model with Log-normal in wtp space<br>\n",
    "* Panel ML with Normal distribution in wtp space<br>\n",
    "* Panel ML with Log-normal distribution in wtp space<br>                     \n",
    "\n",
    "ii. Compare the VTTs of the models with a normal distribution and a log-normal distribution. Do you see a pattern? <br>\n",
    "\n",
    "iii. What could explain this pattern?<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2. Impact of the number of draws on modelling outcomes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Exercise 2: Impact of the number of draws` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the Mixed Logit models that we have estimated, we have used a low number of draws (<100). We choose a relatively low number of draws to avoid long estimation times.  <br>\n",
    "\n",
    "Next, we analyse how sensitive the modelling outcomes are towards the number of draws. To do this, we have estimated a Panel Mixed Logit model using different numbers of draws, ranging from 33 to 2,000, and stored the results. <br>\n",
    "\n",
    "The following plots show the results. \n",
    "\n",
    "![Draws](data/draws_vs_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Questions:`\n",
    "\n",
    "`A` The left-hand side plot shows that the VTT estimate gets more stable with an increasing number of draws. Can you explain why the estimate gets more stable? \n",
    "\n",
    "`B` What number of draws do you deem sufficient for estimating this model? Explain your answer.\n",
    "\n",
    "`C` The right-hand side plot shows a linear relation between the number of draws and the estimation time. Explain why a linear relation was to be expected.\n",
    "\n",
    "`D` Suppose we estimate a model with *K* random parameters. Would the relation between the number of draws and estimation time still be linear? Explain your answer. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the code to create the plot \n",
    "\n",
    "# Create a dataframe to store the results\n",
    "df_out = pd.DataFrame(columns=['num_draws','VTT', 'LL','elapsed_time'])\n",
    "\n",
    "# Define the number of draws to be used for Monte-Carlo simulation\n",
    "num_draws = list(range(33, 201, 33))\n",
    "\n",
    "# Parameters definition enabling the construction of random parameters\n",
    "vtt         = Beta('vtt',       0.4, None, None, 0)\n",
    "B_tc        = Beta('b_tc',     -0.4, None, None, 0)    \n",
    "sigma_vtt   = Beta('sigma_vtt',   2, None, None, 0)\n",
    "\n",
    "# Construction of random parameters   \n",
    "vtt_rnd = exp(vtt + sigma_vtt * bioDraws('vtt_rnd', 'NORMAL_HALTON2'))\n",
    "\n",
    "# Definition of the utility functions \n",
    "V_L = [B_tc * (Variable(f'CostL_{q}') + vtt_rnd * Variable(f'TimeL_{q}')) for q in range(9)]\n",
    "V_R = [B_tc * (Variable(f'CostR_{q}') + vtt_rnd * Variable(f'TimeR_{q}')) for q in range(9)]\n",
    "\n",
    "# Create a dictionary to list the utility functions with the numbering of alternatives\n",
    "V = [{1: V_L[q], 2: V_R[q]} for q in range(9)]\n",
    "        \n",
    "# Create a dictionary to describe the availability conditions of each alternative\n",
    "av = {1:1, 2:1}\n",
    "\n",
    "# The conditional probability of the chosen alternative is a logit\n",
    "condProb = [models.loglogit(V[q], av, Variable(f'Chosen_{q}')) for q in range(9)] \n",
    "\n",
    "# Take the product of the conditional probabilities\n",
    "condprobIndiv = exp(bioMultSum(condProb))   # exp to convert from logP to P again\n",
    "\n",
    "# The unconditional probability is obtained by simulation\n",
    "uncondProb = MonteCarlo(condprobIndiv)\n",
    "\n",
    "# The Log-likelihood is the log of the unconditional probability\n",
    "LL = log(uncondProb)\n",
    "\n",
    "# Loop over the number of draws\n",
    "for R in num_draws:\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Give the model a name\n",
    "    model_name = f'Panel ML WTP space with log-normally distributed vtt with {R} draws'\n",
    "\n",
    "    # Create the Biogeme estimation object containing the data and the model\n",
    "    biogeme = bio.BIOGEME(biodata_wide , LL, number_of_draws=R)\n",
    "    \n",
    "    # Set reporting levels\n",
    "    biogeme.generate_pickle = False\n",
    "    biogeme.generate_html = False\n",
    "    biogeme.save_iterations = False\n",
    "    biogeme.modelName = model_name\n",
    "                                    \n",
    "    # Compute the null loglikelihood for reporting\n",
    "    biogeme.nullLogLike = len(biodata_wide.data)*np.log(1/2)*9\n",
    "\n",
    "    # Estimate the parameters\n",
    "    results = biogeme.estimate()\n",
    "    # print(results.short_summary())\n",
    "\n",
    "    # Get the results in a pandas table\n",
    "    beta_hat = results.get_estimated_parameters()\n",
    "    # print(beta_hat)\n",
    "\n",
    "    # End the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'Elapsed time: {elapsed_time:.2f} seconds\\n\\n')\n",
    "\n",
    "    # Compute the mean value of travel time\n",
    "    mu = beta_hat.loc['vtt']['Value']\n",
    "    sigma = beta_hat.loc['sigma_vtt']['Value'] \n",
    "    mean_lognormal_panel = np.exp(mu + np.square(sigma)/2) * 60\n",
    "    \n",
    "    # Add the results to the dataframe\n",
    "    df_R = pd.DataFrame({'num_draws': [R], 'VTT': [mean_lognormal_panel], 'LL': [results.get_general_statistics()['Final log likelihood'][0]], 'elapsed_time': [elapsed_time]})\n",
    "    df_out = pd.concat([df_out, df_R])\n",
    "\n",
    "# Show the results\n",
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results in a figure\n",
    "fig, ax = plt.subplots(1,3, figsize=(15,5), sharex=True)\n",
    "fig.tight_layout(w_pad=3)\n",
    "\n",
    "ax[0].plot(df_out['num_draws'], df_out['VTT'], marker='.')\n",
    "ax[0].set_xlabel('Number of draws')\n",
    "ax[0].set_ylabel('VTT [euro/hour]')\n",
    "ax[0].set_title('VTT')\n",
    "\n",
    "ax[1].plot(df_out['num_draws'], df_out['LL'], marker='.')\n",
    "ax[1].set_xlabel('Number of draws')\n",
    "ax[1].set_ylabel('Log-likelihood')\n",
    "ax[1].set_title('Log-likelihood')\n",
    "\n",
    "ax[2].plot(df_out['num_draws'], df_out['elapsed_time'], marker='.')\n",
    "ax[2].set_xlabel('Number of draws')\n",
    "ax[2].set_ylabel('Elapsed time [s]')\n",
    "ax[2].set_title('Elapsed time')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
